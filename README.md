# ğŸ“„ PactParser: AI-Powered Contract Intelligence System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green)
![MongoDB](https://img.shields.io/badge/MongoDB-Latest-brightgreen)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)

**An intelligent, scalable system for automated contract parsing and business intelligence extraction**

[Features](#-features) â€¢ [Architecture](#-architecture) â€¢ [Getting Started](#-getting-started) â€¢ [API Documentation](#-api-documentation) â€¢ [Tech Stack](#-tech-stack)

</div>

---

## ğŸ¯ Problem Statement

Modern businesses in AR/SaaS platforms handle hundreds of contracts with varying formats, terms, and structures. Manual review is a critical bottleneckâ€”it's time-consuming, expensive, and error-prone, leading to:

- ğŸ’¸ Missed revenue opportunities
- â° Payment delays and cash flow issues
- âš ï¸ Compliance and legal risks
- ğŸ“Š Poor business intelligence visibility

**PactParser** solves this by providing an automated, AI-powered system that ingests contracts and delivers immediate, actionable intelligence with confidence scoring and gap analysis.

---

## âœ¨ Features

### Core Capabilities

- **ğŸš€ Asynchronous Processing**: Non-blocking upload and background processing using Celery + Redis
- **ğŸ¤– LLM-Powered Extraction**: Leverages Groq's LLaMA-3.3-70B for intelligent data parsing
- **ğŸ“Š Weighted Scoring Algorithm**: Industry-standard 0-100 confidence scoring with gap analysis
- **âš¡ Real-time Status Tracking**: Live progress monitoring with WebSocket-style polling
- **ğŸ—„ï¸ Robust Data Storage**: MongoDB-based persistence with optimized indexing
- **ğŸ³ Fully Dockerized**: One-command deployment with docker-compose
- **ğŸ“± Interactive UI**: Real-time dashboard with contract management and detailed views

### Data Extraction

PactParser extracts critical business intelligence including:

- **Party Identification**: Legal entities, signatories, and roles
- **Financial Details**: Line items, totals, currencies, and tax information
- **Payment Structure**: Terms, schedules, methods, and banking details
- **Revenue Classification**: Recurring vs. one-time, billing cycles, renewal terms
- **Service Level Agreements**: Metrics, penalties, and support terms
- **Account Information**: Billing and technical contacts

---

## ğŸ—ï¸ Architecture

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚
â”‚  (User UI)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit Frontend â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   (Port 8501)       â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
       â”‚ REST API               â”‚
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   FastAPI Backend   â”‚         â”‚ Real-time
â”‚   (Port 8000)       â”‚         â”‚ Polling
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜         â”‚
   â”‚                â”‚           â”‚
   â”‚ Write          â”‚ Dispatch  â”‚
   â–¼                â–¼           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ MongoDB  â”‚    â”‚  Redis   â”‚   â”‚
â”‚ Database â”‚    â”‚  Queue   â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚
                      â”‚         â”‚
                      â–¼         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
              â”‚   Celery    â”‚  â”‚
              â”‚   Worker    â”‚  â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚         â”‚
                     â–¼         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
              â”‚   Groq LLM  â”‚  â”‚
              â”‚   API       â”‚  â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚         â”‚
              Score & Parse    â”‚
                     â”‚         â”‚
                     â–¼         â”‚
              Update MongoDBâ”€â”€â”€â”˜
```

### Data Flow Pipeline

1. **Upload**: User uploads PDF â†’ FastAPI saves file + creates DB record
2. **Queue**: FastAPI dispatches async task to Celery via Redis
3. **Process**: Celery worker executes 4-stage pipeline:
   - ğŸ“„ Extract text from PDF (30%)
   - ğŸ¤– Parse with LLM (70%)
   - ğŸ“Š Score and analyze gaps (90%)
   - âœ… Save results to MongoDB (100%)
4. **Monitor**: Frontend polls status endpoint for real-time updates
5. **View**: User accesses extracted data and analytics

---

## ğŸš€ Getting Started

### Prerequisites

- **Docker & Docker Compose** (required for containerized deployment)
- **Groq API Key** ([Get one free here](https://console.groq.com))

### Quick Start (Recommended)

**1. Clone the repository**

```bash
git clone <your-repo-url>
cd PactParser
```

**2. Create environment file**

```bash
touch backend/.env
```

**3. Add your Groq API key**
Open `backend/.env` and add:

```env
GROQ_API_KEY=gsk_your_api_key_here
```

**4. Launch the application**

```bash
docker-compose up --build
```

That's it! The entire stack will start automatically.

### Access Points

- **Frontend UI**: http://localhost:8501
- **Backend API**: http://localhost:8000
- **API Docs (Swagger)**: http://localhost:8000/docs
- **API Docs (ReDoc)**: http://localhost:8000/redoc

---

## ğŸ’» Local Development Setup

For development without Docker:

### Prerequisites

- Python 3.11+
- MongoDB running on `localhost:27017`
- Redis running on `localhost:6379`

### Setup Steps

**1. Install dependencies**

```bash
pip install -r requirements.txt
```

**2. Configure environment**
Create `backend/.env`:

```env
GROQ_API_KEY=gsk_your_api_key_here
MONGO_CONNECTION_STRING=mongodb://localhost:27017
REDIS_CONNECTION_STRING=redis://localhost:6379/0
```

**3. Start services** (in separate terminals)

Terminal 1 - Backend API:

```bash
cd backend
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Terminal 2 - Celery Worker:

```bash
cd backend
celery -A app.celery_app.celery_app worker --loglevel=info -P eventlet
```

Terminal 3 - Frontend:

```bash
cd frontend
streamlit run streamlit-app.py
```

---

## ğŸ“š API Documentation

### Endpoints Overview

| Method | Endpoint                   | Description                    |
| ------ | -------------------------- | ------------------------------ |
| `POST` | `/contracts/upload`        | Upload new contract (PDF)      |
| `GET`  | `/contracts/{id}/status`   | Poll processing status         |
| `GET`  | `/contracts/{id}`          | Get extracted contract data    |
| `GET`  | `/contracts`               | List all contracts (paginated) |
| `GET`  | `/contracts/{id}/download` | Download original PDF          |

### Example Usage

**Upload a Contract**

```bash
curl -X POST "http://localhost:8000/contracts/upload" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@contract.pdf"
```

Response:

```json
{
  "contract_id": "a1b2c3d4-5678-90ab-cdef-1234567890ab",
  "filename": "contract.pdf",
  "status": "pending"
}
```

**Check Status**

```bash
curl "http://localhost:8000/contracts/{contract_id}/status"
```

Response:

```json
{
  "contract_id": "a1b2c3d4-5678-90ab-cdef-1234567890ab",
  "status": "processing",
  "progress_percentage": 70,
  "error_message": null
}
```

**Get Extracted Data**

```bash
curl "http://localhost:8000/contracts/{contract_id}"
```

For full interactive documentation, visit: http://localhost:8000/docs

---

## ğŸ“Š Scoring Algorithm

PactParser implements a weighted scoring system (0-100 points):

| Category                   | Weight | Criteria                          |
| -------------------------- | ------ | --------------------------------- |
| **Financial Completeness** | 30 pts | Total value, MRR, line items      |
| **Party Identification**   | 25 pts | Legal names, roles, signatories   |
| **Payment Terms Clarity**  | 20 pts | Terms, schedules, methods         |
| **SLA Definition**         | 15 pts | Metrics, penalties, support terms |
| **Contact Information**    | 10 pts | Billing and technical contacts    |

### Gap Analysis

The system automatically identifies missing critical fields and provides actionable feedback:

```json
{
  "confidence_score": 75,
  "gap_analysis": [
    "Missing detailed line items",
    "SLA section found, but no specific metrics or penalties defined"
  ]
}
```

---

## ğŸ› ï¸ Tech Stack

### Backend

- **FastAPI**: Modern, async Python web framework
- **Celery**: Distributed task queue for async processing
- **Redis**: Message broker and result backend
- **MongoDB**: Document database for flexible schema
- **LangChain**: LLM orchestration framework
- **Groq**: High-performance LLM inference (LLaMA-3.3-70B)
- **PyPDF**: PDF text extraction
- **Pydantic**: Data validation and schema generation

### Frontend

- **Streamlit**: Rapid Python-based web UI framework
- **Pandas**: Data manipulation and display

### Infrastructure

- **Docker & Docker Compose**: Containerization and orchestration
- **Eventlet**: Async I/O for Celery workers

---

## ğŸ“ Project Structure

```
PACTPARSER/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __pycache__/         # Python bytecode cache
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI REST API server
â”‚   â”‚   â”œâ”€â”€ celery_app.py        # Celery configuration & initialization
â”‚   â”‚   â”œâ”€â”€ celery_worker.py     # Background task processing logic
â”‚   â”‚   â”œâ”€â”€ database.py          # MongoDB connection & async helpers
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic data models & schemas
â”‚   â”‚   â”œâ”€â”€ llm_parser.py        # LLM extraction with LangChain
â”‚   â”‚   â””â”€â”€ scoring.py           # Weighted scoring & gap analysis
â”‚   â”œâ”€â”€ uploads/                 # Storage for uploaded PDF files
â”‚   â”œâ”€â”€ .env                     # Environment variables (API keys)
â”‚   â””â”€â”€ .envexample              # Example environment configuration
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit-app.py         # Streamlit web UI with real-time polling
â”‚
â”œâ”€â”€ samples/                     # Test contract PDFs
â”‚   â”œâ”€â”€ sample_contract.pdf      # Standard test contract
â”‚   â”œâ”€â”€ test_contract_missing_parties.pdf
â”‚   â”œâ”€â”€ test_contract_missing_sla.pdf
â”‚   â”œâ”€â”€ test_contract_missing_termination...pdf
â”‚   â””â”€â”€ test_contract_vague_financials.pdf
â”‚
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ .dockerignore                # Docker ignore rules
â”œâ”€â”€ backend.Dockerfile           # Backend container definition
â”œâ”€â”€ docker-compose.yml           # Multi-container orchestration
â”œâ”€â”€ frontend.Dockerfile          # Frontend container definition
â”œâ”€â”€ README.md                    # Project documentation
â””â”€â”€ requirements.txt             # Python dependencies (global)
```

### Key Directories Explained

- **`backend/app/`**: Core application logic with clear separation of concerns
- **`backend/uploads/`**: Persistent storage for uploaded contracts (mounted as Docker volume)
- **`frontend/`**: Lightweight Streamlit UI (can be replaced with React)
- **`samples/`**: Edge-case test contracts for validation and demo purposes

---

## ğŸ¨ UI Walkthrough

### 1. Upload Contract

Drag and drop a PDF contract in the sidebar. Upload is instant with immediate confirmation.

### 2. Real-time Processing

Watch live progress as the system:

- Extracts text (30%)
- Parses with AI (70%)
- Scores and analyzes (90%)
- Completes (100%)

### 3. View Results

Click "Details" to see:

- **Summary Tab**: Revenue classification, payment structure
- **Gap Analysis Tab**: Missing fields and recommendations
- **Financial Tab**: Line items, totals, currency
- **Parties Tab**: Legal entities and contacts
- **SLA Tab**: Service level commitments
- **Raw JSON Tab**: Complete extracted data

### 4. Download Original

One-click download of the original PDF for reference.

---

## ğŸ”§ Configuration

### Environment Variables

**Required:**

- `GROQ_API_KEY`: Your Groq API key

**Optional (auto-configured in Docker):**

- `MONGO_CONNECTION_STRING`: MongoDB connection URL
- `REDIS_CONNECTION_STRING`: Redis connection URL
- `API_BASE_URL`: Backend API URL (for frontend)

---

## ğŸš¦ Performance & Scalability

### Current Capabilities

- **File Size**: Handles contracts up to 50MB
- **Concurrent Processing**: Multiple contracts processed simultaneously
- **Throughput**: ~2-5 contracts per minute (depending on LLM API limits)

### Horizontal Scaling

Scale Celery workers independently:

```bash
docker-compose up --scale backend-worker=5
```

This allows processing 5 contracts in parallel without affecting API responsiveness.

---

## ğŸ¯ Design Decisions

### Why Streamlit Instead of React?

For this MVP, **Streamlit was a strategic choice**:

âœ… **Rapid Development**: Built a fully functional UI in hours vs. days  
âœ… **Python-Native**: Seamless integration with backend logic and data structures  
âœ… **Feature-Rich**: Real-time polling, interactive dialogs, data visualization out-of-the-box  
âœ… **Validation Focus**: Proves the entire data pipeline and business logic work flawlessly

The architecture is **frontend-agnostic**. The REST API can easily support a React/Next.js frontend when production-ready branding and UX customization are needed.

### Why Celery + Redis?

- **Non-blocking**: API remains responsive during heavy processing
- **Reliable**: Redis persistence ensures no task loss
- **Scalable**: Add workers without touching API code
- **Observable**: Built-in monitoring and logging

---

## ğŸ”® Future Enhancements

### Planned Features

- [ ] **React Frontend**: Production-ready UI with custom branding
- [ ] **Map-Reduce Parsing**: Handle contracts > 50MB by chunking
- [ ] **LangSmith Integration**: LLM call tracing and debugging
- [ ] **User Authentication**: OAuth2 with role-based access
- [ ] **Webhook Notifications**: Real-time alerts on completion
- [ ] **Batch Upload**: Process multiple contracts at once
- [ ] **Export to Excel**: Structured data export functionality
- [ ] **Unit Tests**: 60%+ coverage with pytest

### Advanced AI Features

- [ ] **Agentic Parsing**: Multi-step reasoning for complex contracts
- [ ] **Custom Fine-tuning**: Domain-specific model optimization
- [ ] **Clause Comparison**: Detect deviations from standard terms
- [ ] **Risk Scoring**: Identify potentially unfavorable clauses

---

## ğŸ› Troubleshooting

### Common Issues

**"Connection Error: Cannot connect to backend API"**

- Ensure all Docker containers are running: `docker-compose ps`
- Check backend logs: `docker-compose logs backend-api`

**"Failed to connect to MongoDB"**

- MongoDB container may not be ready. Wait 10 seconds and refresh.
- Check MongoDB logs: `docker-compose logs mongo`

**"LLM parsing failed"**

- Verify `GROQ_API_KEY` is set correctly in `backend/.env`
- Check API quota: https://console.groq.com

**"File upload fails"**

- Ensure file is a valid PDF
- Check file size < 50MB
- Verify `uploads/` directory exists with write permissions

### Reset Everything

```bash
docker-compose down -v
docker-compose up --build
```

---

## ğŸ“ Testing

### Manual Testing Checklist

- [ ] Upload a valid PDF contract
- [ ] Verify real-time status updates
- [ ] Check extracted data completeness
- [ ] Download original file
- [ ] Test with malformed PDF
- [ ] Test with non-PDF file
- [ ] Test concurrent uploads

### Sample Contracts

Test with various contract types:

- SaaS subscription agreements
- Professional services contracts
- License agreements
- Master service agreements (MSAs)

---

## ğŸ¤ Contributing

This is a technical assignment project. For production use:

1. Fork the repository
2. Create a feature branch
3. Implement changes with tests
4. Submit a pull request

---

## ğŸ“„ License

This project was created as a technical assignment demonstration.

---

## ğŸ‘¨â€ğŸ’» Author

Built with âš¡ using Python, FastAPI, Celery, and Groq LLM

**Tech Stack Philosophy**: Python-first development using modern frameworks (FastAPI, Streamlit) for rapid, maintainable solutions.

---

## ğŸ™ Acknowledgments

- **Groq** for lightning-fast LLM inference
- **LangChain** for LLM orchestration patterns
- **FastAPI** for modern Python web development
- **Streamlit** for rapid UI prototyping

---

<div align="center">

**Built for the Contract Intelligence Parser Technical Assignment**

Made with â¤ï¸ and â˜•

</div>
